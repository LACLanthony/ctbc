{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 英文詞頻矩陣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#?CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "content = [\"How to format my hard disk\", \" Hard disk format problems \"]\n",
    "X = vectorizer.fit_transform(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['disk', 'format', 'hard', 'how', 'my', 'problems', 'to']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1 0 1]\n",
      " [1 1 1 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = 'data/toy/'\n",
    "posts = []\n",
    "for fname in os.listdir(path):\n",
    "    posts.append(open(path + fname).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is a toy post about machine learning. Actually, it contains not much interesting stuff.',\n",
       " 'Imaging databases provide storage capabilities.',\n",
       " 'Most imaging databases safe images permanently.',\n",
       " 'Imaging databases store data.',\n",
       " 'Imaging databases store data. Imaging databases store data. Imaging databases store data.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = ['This is a toy post about machine learning. Actually, it contains not much interesting stuff.',\n",
    " 'Imaging databases provide storage capabilities.',\n",
    " 'Most imaging databases safe images permanently.',\n",
    " 'Imaging databases store data.',\n",
    " 'Imaging databases store data. Imaging databases store data. Imaging databases store data.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectorizer.fit_transform(posts)\n",
    "num_samples, num_features = X_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 25)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['about', 'actually', 'capabilities', 'contains', 'data', 'databases', 'images', 'imaging', 'interesting', 'is', 'it', 'learning', 'machine', 'most', 'much', 'not', 'permanently', 'post', 'provide', 'safe', 'storage', 'store', 'stuff', 'this', 'toy']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "        1, 1, 1],\n",
       "       [0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "        0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0],\n",
       "       [0, 0, 0, 0, 3, 3, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3,\n",
       "        0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_post = 'qoo imaging database'\n",
    "new_post_vec = vectorizer.transform([new_post])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_post_vec.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(new_post_vec.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "\n",
    "# euclidean\n",
    "# sqrt(sum((v1 - v2) ** 2))\n",
    "\n",
    "def dist_raw(v1, v2):\n",
    "    delta = v1 - v2\n",
    "    return sp.linalg.norm(delta.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4142135623730951"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "a  = np.array([0,0,1,1,0])\n",
    "b  = np.array([1,0,1,1,1])\n",
    "np.sqrt(sum((a - b) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4142135623730951"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.linalg.norm( (a - b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a toy post about machine learning. Actually, it contains not much interesting stuff. 3.872983346207417\n",
      "Imaging databases provide storage capabilities. 2.0\n",
      "Most imaging databases safe images permanently. 2.23606797749979\n",
      "Imaging databases store data. 1.7320508075688772\n",
      "Imaging databases store data. Imaging databases store data. Imaging databases store data. 5.5677643628300215\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, num_samples):\n",
    "    post_vec = X_train.getrow(i)\n",
    "    d = dist_raw(post_vec, new_post_vec)\n",
    "    print(posts[i],d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(v1, v2):\n",
    "    v1_normalized  = v1 / sp.linalg.norm(v1.toarray()) \n",
    "    v2_normalized  = v2 / sp.linalg.norm(v2.toarray())\n",
    "    delta = v1_normalized - v2_normalized\n",
    "    return sp.linalg.norm(delta.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 小內閣之一是她？ 「王淺秋」內定高市新聞局長\n",
    "# 韓國瑜要在一個月內完成小內閣組閣，局處首長人選，他說目前找到了一半，至於名單有誰，韓國瑜說直接講名字很敏感，會在就職日公布，不過前一天，韓國瑜北上參加私人餐會，媒體在現場等待時間，六福集團的公關協理王淺秋儼然像公關，在場當協調橋樑，加上王淺秋過去就有新聞媒體背景，外傳將接任高雄市新聞局長。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 小內閣 王淺秋 高市 新聞局長\n",
    "# 小內閣 王淺秋 高市 新聞局長 韓國瑜 首長 人選 一半 名單 敏感..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0])\n",
    "b = np.array([1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1])\n",
    "delta = a - b\n",
    "sp.linalg.norm(delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0514622242382672"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0])\n",
    "b = np.array([1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1])\n",
    "a_normalized  = a / sp.linalg.norm(a) \n",
    "b_normalized  = b / sp.linalg.norm(b)\n",
    "delta = a_normalized - b_normalized\n",
    "sp.linalg.norm(delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a toy post about machine learning. Actually, it contains not much interesting stuff. 1.4142135623730951\n",
      "Imaging databases provide storage capabilities. 1.0514622242382672\n",
      "Most imaging databases safe images permanently. 1.0878894332937856\n",
      "Imaging databases store data. 1.0\n",
      "Imaging databases store data. Imaging databases store data. Imaging databases store data. 1.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, num_samples):\n",
    "    post_vec = X_train.getrow(i)\n",
    "    d = dist(post_vec, new_post_vec)\n",
    "    print(posts[i],d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['actually', 'capabilities', 'contains', 'data', 'databases', 'images', 'imaging', 'interesting', 'learning', 'machine', 'permanently', 'post', 'provide', 'safe', 'storage', 'store', 'stuff', 'toy']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X_train = vectorizer.fit_transform(posts)\n",
    "print(vectorizer.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['noone',\n",
       " 'our',\n",
       " 'next',\n",
       " 'her',\n",
       " 'other',\n",
       " 'formerly',\n",
       " 'around',\n",
       " 'fifteen',\n",
       " 'rather',\n",
       " 'nevertheless']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vectorizer.get_stop_words())[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_post = 'qoo imaging database'\n",
    "new_post_vec = vectorizer.transform([new_post])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a toy post about machine learning. Actually, it contains not much interesting stuff. 1.4142135623730951\n",
      "Imaging databases provide storage capabilities. 1.0514622242382672\n",
      "Most imaging databases safe images permanently. 1.0514622242382672\n",
      "Imaging databases store data. 1.0\n",
      "Imaging databases store data. Imaging databases store data. Imaging databases store data. 1.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, num_samples):\n",
    "    post_vec = X_train.getrow(i)\n",
    "    d = dist(post_vec, new_post_vec)\n",
    "    print(posts[i],d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages\n",
      "Requirement already satisfied: six in /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages (from nltk)\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.stem\n",
    "s = nltk.stem.SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'graphic'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.stem('graphics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'imag'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.stem(\"imaging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'imag'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.stem(\"image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'imagin'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.stem(\"imagination\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'imagin'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.stem(\"imagine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.stem\n",
    "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['actual', 'capabl', 'contain', 'data', 'databas', 'imag', 'interest', 'learn', 'machin', 'perman', 'post', 'provid', 'safe', 'storag', 'store', 'stuff', 'toy']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = StemmedCountVectorizer(stop_words='english')\n",
    "X_train = vectorizer.fit_transform(posts)\n",
    "print(vectorizer.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_post = 'qoo imaging database'\n",
    "new_post_vec = vectorizer.transform([new_post])\n",
    "new_post_vec.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a toy post about machine learning. Actually, it contains not much interesting stuff. 1.414213562373095\n",
      "Imaging databases provide storage capabilities. 0.8573732768944039\n",
      "Most imaging databases safe images permanently. 0.6296288974669553\n",
      "Imaging databases store data. 0.7653668647301795\n",
      "Imaging databases store data. Imaging databases store data. Imaging databases store data. 0.7653668647301795\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, num_samples):\n",
    "    post_vec = X_train.getrow(i)\n",
    "    d = dist(post_vec, new_post_vec)\n",
    "    print(posts[i],d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(TfidfVectorizer,self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['actual', 'capabl', 'contain', 'data', 'databas', 'imag', 'interest', 'learn', 'machin', 'perman', 'post', 'provid', 'safe', 'storag', 'store', 'stuff', 'toy']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = StemmedTfidfVectorizer(stop_words='english')\n",
    "X_train = vectorizer.fit_transform(posts)\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_post = 'qoo imaging database'\n",
    "new_post_vec = vectorizer.transform([new_post])\n",
    "#new_post_vec.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a toy post about machine learning. Actually, it contains not much interesting stuff. 1.4142135623730951\n",
      "Imaging databases provide storage capabilities. 1.0789758507558254\n",
      "Most imaging databases safe images permanently. 0.859044512133176\n",
      "Imaging databases store data. 0.924634506718001\n",
      "Imaging databases store data. Imaging databases store data. Imaging databases store data. 0.924634506718001\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, num_samples):\n",
    "    post_vec = X_train.getrow(i)\n",
    "    d = dist(post_vec, new_post_vec)\n",
    "    print(posts[i],d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 中文詞頻矩陣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['柯文哲 為 了 大巨蛋 一事 找 趙藤雄 算帳', '柯P 將不在 大巨蛋 舉辦 世運會']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba\n",
    "jieba.load_userdict('userdict.txt')\n",
    "\n",
    "a = ['柯文哲為了大巨蛋一事找趙藤雄算帳', \n",
    "     '柯P將不在大巨蛋舉辦世運會']\n",
    "\n",
    "corpus = []\n",
    "for s in a:\n",
    "    corpus.append(' '.join(jieba.cut(s)))\n",
    "corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['柯文哲 為 了 大巨蛋 一事 找 趙藤雄 算帳', '柯P 將不在 大巨蛋 舉辦 世運會']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [' '.join(jieba.cut(s)) for s in a]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['一事', '世運會', '大巨蛋', '將不在', '柯p', '柯文哲', '算帳', '舉辦', '趙藤雄']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 0, 0, 1, 1, 0, 1],\n",
       "       [0, 1, 1, 1, 1, 0, 0, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "term = '柯文哲'\n",
    "res = requests.get('https://zh.wikipedia.org/wiki/{}'.format(term))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['柯文哲', 'Ko Wen-je', '柯P', 'KP']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(res.text, 'lxml')\n",
    "synonyms = []\n",
    "for p in soup.select('p'):\n",
    "    if p.select_one('span'):\n",
    "        synonyms.extend([w.text for w in p.select('b')])\n",
    "synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('synonym.txt', 'w') as f:\n",
    "    f.write('/'.join(synonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ko wen-je': '柯文哲', 'kp': '柯文哲', '柯p': '柯文哲'}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synoym_dic = {}\n",
    "for s in open('synonym.txt', 'r'):\n",
    "    words = s.split('/')\n",
    "    for w in words[1:]:\n",
    "        synoym_dic[w.lower()] = words[0]\n",
    "synoym_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SynonymCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(SynonymCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (synoym_dic.get(w, w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['一事', '世運會', '大巨蛋', '將不在', '柯文哲', '算帳', '舉辦', '趙藤雄']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = SynonymCountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 0, 1, 1, 0, 1],\n",
       "       [0, 1, 1, 1, 1, 0, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['世運會', '大巨蛋', '將不在', '柯文哲', '算帳', '舉辦', '趙藤雄']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = SynonymCountVectorizer(stop_words=['一事'])\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 中文停用字詞\n",
    "- https://github.com/tomlinNTUB/Python-in-5-days/blob/master/10-2%20%E4%B8%AD%E6%96%87%E6%96%B7%E8%A9%9E-%E7%A7%BB%E9%99%A4%E5%81%9C%E7%94%A8%E8%A9%9E.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "s = pandas.read_clipboard(header = None)\n",
    "stopwords = s[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 計算文章相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "news = pandas.read_excel('https://raw.githubusercontent.com/ywchiu/ctbc/master/data/appledaily_hw.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corpus = []\n",
    "titles = []\n",
    "for rec in news.iterrows():\n",
    "    titles.append(rec[1].title)\n",
    "    corpus.append(' '.join(jieba.cut(rec[1].content)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "class ChineseCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(ChineseCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (w for w in analyzer(doc) if re.match('[\\u4e00-\\u9fa5]+', w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = ChineseCountVectorizer(stop_words=stopwords)\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<899x36095 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 113215 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "cs = cosine_distances(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(899, 899)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'【更新】水利會改官派明闖關\\u3000綠委24小時前顧議場大門防藍突襲'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "反對水利會改制　吳敦義下令：藍委做好夜宿立院抗爭準備 0.6249319023845766\n",
      "農田水利會改公務機關　蔡英文：這不是綁樁 0.6650818913541277\n",
      "【搏感情動畫】台電每年30億敦親睦鄰費　協助立委選民服務 0.7763414418773449\n",
      "在野黨突襲表決《勞基法》修正案無效　民進黨烙人成功擋下 0.7785595271207466\n",
      "罷免案將投票　李遠哲今再度現身力挺黃國昌 0.7821725817618267\n",
      "國防部解約慶富要提告　馮世寬怒嗆三遍：請便 0.787735535313332\n"
     ]
    }
   ],
   "source": [
    "for idx in cs[0].argsort()[1:11]:\n",
    "    if cs[0][idx] < 0.8:\n",
    "        print(titles[idx], cs[0][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSimiliarArticle(pos):\n",
    "    print('查詢文章:', titles[pos])\n",
    "    for idx in cs[pos].argsort()[1:11]:\n",
    "        if cs[pos][idx] < 0.8:\n",
    "            print('相關文章:',titles[idx], cs[pos][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查詢文章: 黃國昌：長期目標要消滅國民黨　不支持柯文哲的兩岸一家親\n",
      "相關文章: 「兩蔣時代」超譯　網友：不准傷皇城內的和氣！ 0.573991318811758\n",
      "相關文章: 罷免案將投票　李遠哲今再度現身力挺黃國昌 0.5856440615013038\n",
      "相關文章: 罷昌案周六投票　時代力量全力澄清不實謠言 0.629022271791837\n",
      "相關文章: 黃國昌若被罷免會更強　他：成為選輸北市的阿扁 0.6310238769836006\n",
      "相關文章: 【聲援片】沈發惠呼籲罷昌案投「不同意」　黃國昌：謝謝您 0.7457647744915028\n"
     ]
    }
   ],
   "source": [
    "getSimiliarArticle(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "csw = cosine_distances(X.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36095, 36095)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "#titles[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12028"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names().index('小嫻')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "何守 0.057847605573501104\n",
      "密友 0.24791062532467667\n",
      "何守正 0.2503553599400147\n",
      "健身房 0.2884303103522792\n",
      "信妙 0.3126823393798195\n",
      "分居 0.34939790478943866\n",
      "宮導致 0.3514046374402636\n",
      "何家 0.3578979193561096\n",
      "獨子 0.36503471393081455\n",
      "不孕 0.36655934526106637\n"
     ]
    }
   ],
   "source": [
    "for idx in csw[12028].argsort()[1:11]:\n",
    "    print(vectorizer.get_feature_names()[idx], csw[12028][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "csw[19153].sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文字分群"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "news = pandas.read_excel('https://raw.githubusercontent.com/ywchiu/ctbc/master/data/20150628news.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "titles = []\n",
    "for rec in news.iterrows():\n",
    "    corpus.append(' '.join(jieba.cut(rec[1]['description'])))\n",
    "    titles.append(rec[1]['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<147x12827 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 23782 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cs = cosine_similarity(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(147, 147)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cluster\n",
    "km = cluster.KMeans(n_clusters=4, init='k-means++', random_state=42)\n",
    "c = km.fit_predict(cs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "希臘國內三分一自動櫃員機現金短缺\n",
      "歐元區財長拒希臘延長救助計劃\n",
      "呂紹煒專欄：違約與退出 希臘與歐洲才能重生(上)\n",
      "希臘違約在即  歐盟全力穩定經濟\n",
      "希臘脫歐變可能 歐洲衝擊大\n",
      "希債協議  法國願盡最後斡旋努力\n",
      "希臘1／3提款機錢被提光\n",
      "確保銀行穩定 希臘續與ECB緊密合作\n",
      "希臘態度強硬 歐元區耐心漸失\n",
      "希臘盼展延債務 歐元區拒絕\n"
     ]
    }
   ],
   "source": [
    "np_titles = np.array(titles)\n",
    "for rec in np_titles[c==3]:\n",
    "    print(rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 任務：\n",
    "- 請將資料集 20171214news.xlsx (https://raw.githubusercontent.com/ywchiu/ctbc/master/data/20171214news.xlsx) 分成20群並檢視每一群的分群結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "news = pandas.read_excel('https://raw.githubusercontent.com/ywchiu/ctbc/master/data/20171214news.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>content</th>\n",
       "      <th>link</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>政治</td>\n",
       "      <td>新增：立委說法民進黨立法院黨團預計在明天的院會中，讓改制農田水利會的《農田水利會組織通則》修...</td>\n",
       "      <td>https://tw.news.appledaily.com/politics/realti...</td>\n",
       "      <td>【更新】水利會改官派明闖關　綠委24小時前顧議場大門防藍突襲</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>論壇</td>\n",
       "      <td>邱俊棠／台中市民、見習醫師；曾任台灣醫學生聯合會對外副會長對於公民參與公眾事務而能得到單位首...</td>\n",
       "      <td>https://tw.news.appledaily.com/forum/realtime/...</td>\n",
       "      <td>請中市府為所當為 加速中火燃煤限制</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>社會</td>\n",
       "      <td>被控來台涉發展情報組織的中國學生周泓旭，因接觸我方外交部官員而露餡落網，今年9月被台北地院一...</td>\n",
       "      <td>https://tw.news.appledaily.com/local/realtime/...</td>\n",
       "      <td>陸生共諜嗆台司法　「不敢公開審理我」</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>生活</td>\n",
       "      <td>【廣編特輯】 對於全能神教會在中國所遭受的迫害與在韓國、香港及臺灣所面對的反面宣傳，非政府組...</td>\n",
       "      <td>https://tw.news.appledaily.com/life/realtime/2...</td>\n",
       "      <td>【特企】NGO聯名譴責中共迫害宗教信仰</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>生活</td>\n",
       "      <td>立法院交通委員會今審查《道路交通管理處罰條例》，通過營業大客車駕駛人行駛一般道路未繫安全帶，...</td>\n",
       "      <td>https://tw.news.appledaily.com/life/realtime/2...</td>\n",
       "      <td>乘客搭大客車不配合繫安全帶　立院初審通過可罰6千元</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  category                                            content  \\\n",
       "0       政治  新增：立委說法民進黨立法院黨團預計在明天的院會中，讓改制農田水利會的《農田水利會組織通則》修...   \n",
       "1       論壇  邱俊棠／台中市民、見習醫師；曾任台灣醫學生聯合會對外副會長對於公民參與公眾事務而能得到單位首...   \n",
       "2       社會  被控來台涉發展情報組織的中國學生周泓旭，因接觸我方外交部官員而露餡落網，今年9月被台北地院一...   \n",
       "3       生活  【廣編特輯】 對於全能神教會在中國所遭受的迫害與在韓國、香港及臺灣所面對的反面宣傳，非政府組...   \n",
       "4       生活  立法院交通委員會今審查《道路交通管理處罰條例》，通過營業大客車駕駛人行駛一般道路未繫安全帶，...   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://tw.news.appledaily.com/politics/realti...   \n",
       "1  https://tw.news.appledaily.com/forum/realtime/...   \n",
       "2  https://tw.news.appledaily.com/local/realtime/...   \n",
       "3  https://tw.news.appledaily.com/life/realtime/2...   \n",
       "4  https://tw.news.appledaily.com/life/realtime/2...   \n",
       "\n",
       "                            title  \n",
       "0  【更新】水利會改官派明闖關　綠委24小時前顧議場大門防藍突襲  \n",
       "1               請中市府為所當為 加速中火燃煤限制  \n",
       "2              陸生共諜嗆台司法　「不敢公開審理我」  \n",
       "3             【特企】NGO聯名譴責中共迫害宗教信仰  \n",
       "4       乘客搭大客車不配合繫安全帶　立院初審通過可罰6千元  "
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "titles = []\n",
    "for rec in news.iterrows():\n",
    "    corpus.append(' '.join(jieba.cut(rec[1]['content'])))\n",
    "    titles.append(rec[1]['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cs = cosine_similarity(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "fit_predict() missing 1 required positional argument: 'X'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-211-09824fd7e881>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mkm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'k-means++'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: fit_predict() missing 1 required positional argument: 'X'"
     ]
    }
   ],
   "source": [
    "from sklearn import cluster\n",
    "km = cluster.KMeans(n_clusters=20, init='k-means++', random_state=42)\n",
    "c = km.fit_predict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "小嫻婚變冒毒菇　勾于美人「奉茶」夢魘\n",
      "明明是好康分享文　網友卻瘋喊噁心慎入\n",
      "「想抱孫要看天！」　他批小嫻婆婆殘忍強逼\n",
      "【好聚不好散】離婚掏出6千萬　這女星比小嫻還慘\n",
      "【狗仔偷拍】小嫻搬離何守正家租66坪房　月租6萬元\n",
      "許聖梅心疼小嫻被當空氣　爆何守正「有兩個女學員」\n",
      "【動畫解盤】毒菇跳火線譙seafood　小嫻難瘦香菇\n",
      "十二月十四日各報頭條搶先報\n",
      "不捨善良小嫻慘遭婚變　乃哥「命運捉弄人」\n",
      "【獨家】小嫻賣房求子　婆婆竟拒入籍何家\n",
      "【毒菇護弟】何守正姊姊不是華岡七仙女　美法連線批小嫻\n",
      "小嫻離婚導火線　拉何守正信妙禪\n",
      "【狗仔偷拍】何守正現身！「全台最沒尊嚴的婆婆」也出來了\n",
      "小嫻中分手魔咒！ 同公司4女星全都婚變\n",
      "他因為小嫻婚變被分手　網友跪求：拜託別放生\n",
      "潔哥目睹「正嫻之變」　驚呼：靠北系列竟是真的\n",
      "何守正姐姐神護航！遭網友酸：全台最討人厭大姑\n",
      "小嫻別傻傻被欺負！女律師說「姐寶」就要這樣對付\n",
      "【小嫻婚變】他說很奇怪　「一定是男生劈腿？」\n",
      "教友小嫻婚姻觸礁　曾之喬談情避不開Seafood\n",
      "小嫻守正結婚在台沒登記　想離婚只有兩條路\n",
      "胡瓜2個月前耳聞小嫻婚變　震驚之餘好心疼\n",
      "昔日搭檔談小嫻婚變　曾國城這樣說\n",
      "何守正兩個姊姊護航扯婆媳　「他」戳破媽寶特色\n",
      "大姑出面護弟！轟小嫻不能生「媽媽是全台最沒有尊嚴的婆婆」\n",
      "小嫻信奉妙禪　關鍵原因與何守正有關！\n",
      "小嫻何守正想離婚　必須先做這件事！\n",
      "【K律師論點】離婚＝失敗？　K律師這麼說\n",
      "真尷尬！他只是聊個天　正妹就退出對話了\n",
      "女網紅因為這理由挺余祥銓！讓人不知該哭還是笑\n",
      "「小嫻不快樂！」　許聖梅：何守正虧欠她\n",
      "一下車有人墜樓掉在車頂　網友：車牌有密碼\n",
      "【內幕動畫】小嫻婚變何守正姊反擊　不滿媽煮飯侍奉星媳婦\n",
      "小嫻多信妙禪？　曾見證「師父帶我跳舞」\n",
      "【話當年】被拍和她上賓館　何守正掰了阿妹\n",
      "小嫻婚變無徵兆　男星嘆：兩人向來出雙入對\n",
      "【獨家內幕】太傷！小嫻被分手　何守正當小三面前攤牌\n",
      "【小嫻離婚】何守正稱沒有遺憾　人妻女星超火「一嘴屁話」\n",
      "【有片】何守正小嫻驚爆離婚　健身房中嗅出端倪\n",
      "【小嫻離婚】3大退讓人財兩失　求子花光430萬積蓄\n"
     ]
    }
   ],
   "source": [
    "np_titles = np.array(titles)\n",
    "for rec in np_titles[c==0]:\n",
    "    print(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "sil_ary = []\n",
    "for k in range(2,21):\n",
    "    km = cluster.KMeans(n_clusters = k, init='k-means++', random_state=42)\n",
    "    c = km.fit_predict(cs)\n",
    "    sil_ary.append({'group':k,\n",
    "                    'silhouette':silhouette_score(cs, labels=c)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['f', 'rec', 'dist']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x118a4f278>"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "% pylab inline\n",
    "import pandas\n",
    "sil_df = pandas.DataFrame(sil_ary)\n",
    "sil_df.plot(kind = 'line', x='group', y='silhouette')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cs = cosine_similarity(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(899, 899)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "m = (cs >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.from_numpy_matrix(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-louvain in /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages\n",
      "Requirement already satisfied: networkx in /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages (from python-louvain)\n",
      "Requirement already satisfied: decorator>=4.1.0 in /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages (from networkx->python-louvain)\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install python-louvain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "import community\n",
    "comm = community.best_partition(G)\n",
    "cluster_ary = np.array(list(comm.values()) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_ary  = np.array(titles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "70\n",
      "【狗仔偷拍】小嫻搬離何守正家租66坪房　月租6萬元\n",
      "許聖梅心疼小嫻被當空氣　爆何守正「有兩個女學員」\n",
      "【動畫解盤】毒菇跳火線譙seafood　小嫻難瘦香菇\n",
      "【獨家】小嫻賣房求子　婆婆竟拒入籍何家\n",
      "小心！在美結婚台灣沒登記　偷腥照樣能捉姦\n",
      "小嫻離婚導火線　拉何守正信妙禪\n",
      "小嫻守正結婚在台沒登記　想離婚只有兩條路\n",
      "小嫻何守正想離婚　必須先做這件事！\n",
      "【內幕動畫】小嫻婚變何守正姊反擊　不滿媽煮飯侍奉星媳婦\n",
      "小嫻婚變無徵兆　男星嘆：兩人向來出雙入對\n",
      "【獨家內幕】太傷！小嫻被分手　何守正當小三面前攤牌\n",
      "【小嫻離婚】何守正稱沒有遺憾　人妻女星超火「一嘴屁話」\n",
      "【小嫻離婚】3大退讓人財兩失　求子花光430萬積蓄\n",
      "114\n",
      "驚！　野生捕獲恐龍遛汪星人\n",
      "驚！　野生捕獲恐龍遛汪星人\n",
      "搔癢無尾熊 表情超enjoy\n",
      "【笑翻片】各種抓！動物止癢出奇招\n",
      "【保護片】就是不能下水！狗狗護小主人\n",
      "群狗亂舞開趴　畫面太「美」讓人不敢看\n",
      "超近看螞蟻食蜂蜜　竟感到療癒？\n",
      "201\n",
      "被爆罵、敲台電　顏寬恒「模糊反空污焦點」\n",
      "【反嗆片】被爆向台電申請補助又罵空污　顏寬恒：不會拿人手短\n",
      "「一手罵台電、一手敲台電」　經委會要台電公布立委補助資料\n",
      "被顏寬恒點名也跟台電要補助　蔡其昌：不應與空污混為一談\n",
      "341\n",
      "【壹週刊】百億貸款將到期　柯P放容積解套京華城\n",
      "560%容積爭議　威京：只是拿回應有的\n",
      "京華城容積率增至560%案　北市都發局同意、待都委會審議\n",
      "及時雨！柯文哲放寬容積　京華城爽納百億\n",
      "17\n",
      "【不斷更新】桃園工廠惡火撲滅 6人仍失聯宿舍內發現一堆白骨\n",
      "桃園工廠火勢熄滅　員工宿舍內發現一堆白骨\n",
      "汽車用品大廠「矽卡」燒毀　資本額達2億\n",
      "45\n",
      "後天入冬最強冷空氣來襲　低溫下探12℃\n",
      "像灑了糖霜！玉山今晨降雪　積雪0.5公分\n",
      "輕颱啟德恐生成　入冬最強冷空氣周末來襲下探11℃（動畫）\n",
      "185\n",
      "【崩潰動畫】《天堂M》斷線50次　玩家罵聲連連\n",
      "台戰豪砸40萬抽寶一場空　怒批《天堂M》根本錢坑\n",
      "《天堂M》超過116萬人登入　50個伺服器仍塞到爆\n",
      "246\n",
      "【世界一瞬間】日本扭蛋風潮\n",
      "曾銘宗爆「好大的高雄銀」　高雄銀嗆：保留法律追訴權\n",
      "黃國昌爆高雄銀替慶富不實增資　公司：非事實\n",
      "524\n",
      "三中案馬英九圖利中時老闆？　北檢：不評論\n",
      "三中案祕錄光碟！恐扳倒馬英九　當年他錄的\n",
      "三中案500萬禮盒照　馬英九一句話成關鍵\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "c = Counter(cluster_ary)\n",
    "for group, cnt in c.most_common(10):\n",
    "    print(group)\n",
    "    articles = titles_ary[cluster_ary == group]\n",
    "    for news in articles:\n",
    "        print(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 30),\n",
       " (70, 13),\n",
       " (114, 7),\n",
       " (201, 4),\n",
       " (341, 4),\n",
       " (17, 3),\n",
       " (45, 3),\n",
       " (185, 3),\n",
       " (246, 3),\n",
       " (524, 3),\n",
       " (566, 3),\n",
       " (644, 3),\n",
       " (19, 2),\n",
       " (55, 2),\n",
       " (56, 2),\n",
       " (68, 2),\n",
       " (107, 2),\n",
       " (110, 2),\n",
       " (112, 2),\n",
       " (135, 2),\n",
       " (139, 2),\n",
       " (140, 2),\n",
       " (141, 2),\n",
       " (142, 2),\n",
       " (143, 2),\n",
       " (153, 2),\n",
       " (155, 2),\n",
       " (167, 2),\n",
       " (186, 2),\n",
       " (193, 2),\n",
       " (194, 2),\n",
       " (195, 2),\n",
       " (207, 2),\n",
       " (213, 2),\n",
       " (248, 2),\n",
       " (323, 2),\n",
       " (334, 2),\n",
       " (336, 2),\n",
       " (360, 2),\n",
       " (361, 2),\n",
       " (363, 2),\n",
       " (384, 2),\n",
       " (434, 2),\n",
       " (440, 2),\n",
       " (441, 2),\n",
       " (442, 2),\n",
       " (443, 2),\n",
       " (444, 2),\n",
       " (468, 2),\n",
       " (538, 2),\n",
       " (570, 2),\n",
       " (594, 2),\n",
       " (740, 2),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (4, 1),\n",
       " (5, 1),\n",
       " (6, 1),\n",
       " (7, 1),\n",
       " (8, 1),\n",
       " (9, 1),\n",
       " (10, 1),\n",
       " (11, 1),\n",
       " (12, 1),\n",
       " (13, 1),\n",
       " (14, 1),\n",
       " (15, 1),\n",
       " (16, 1),\n",
       " (18, 1),\n",
       " (20, 1),\n",
       " (21, 1),\n",
       " (22, 1),\n",
       " (23, 1),\n",
       " (24, 1),\n",
       " (25, 1),\n",
       " (26, 1),\n",
       " (27, 1),\n",
       " (28, 1),\n",
       " (29, 1),\n",
       " (30, 1),\n",
       " (31, 1),\n",
       " (32, 1),\n",
       " (33, 1),\n",
       " (34, 1),\n",
       " (35, 1),\n",
       " (36, 1),\n",
       " (37, 1),\n",
       " (38, 1),\n",
       " (39, 1),\n",
       " (40, 1),\n",
       " (41, 1),\n",
       " (42, 1),\n",
       " (43, 1),\n",
       " (44, 1),\n",
       " (46, 1),\n",
       " (47, 1),\n",
       " (48, 1),\n",
       " (49, 1),\n",
       " (50, 1),\n",
       " (51, 1),\n",
       " (52, 1),\n",
       " (53, 1),\n",
       " (54, 1),\n",
       " (57, 1),\n",
       " (58, 1),\n",
       " (59, 1),\n",
       " (60, 1),\n",
       " (61, 1),\n",
       " (62, 1),\n",
       " (63, 1),\n",
       " (64, 1),\n",
       " (65, 1),\n",
       " (66, 1),\n",
       " (67, 1),\n",
       " (69, 1),\n",
       " (71, 1),\n",
       " (72, 1),\n",
       " (73, 1),\n",
       " (74, 1),\n",
       " (75, 1),\n",
       " (76, 1),\n",
       " (77, 1),\n",
       " (78, 1),\n",
       " (79, 1),\n",
       " (80, 1),\n",
       " (81, 1),\n",
       " (82, 1),\n",
       " (83, 1),\n",
       " (84, 1),\n",
       " (85, 1),\n",
       " (86, 1),\n",
       " (87, 1),\n",
       " (88, 1),\n",
       " (89, 1),\n",
       " (90, 1),\n",
       " (91, 1),\n",
       " (92, 1),\n",
       " (93, 1),\n",
       " (94, 1),\n",
       " (95, 1),\n",
       " (96, 1),\n",
       " (97, 1),\n",
       " (98, 1),\n",
       " (99, 1),\n",
       " (100, 1),\n",
       " (101, 1),\n",
       " (102, 1),\n",
       " (103, 1),\n",
       " (104, 1),\n",
       " (105, 1),\n",
       " (106, 1),\n",
       " (108, 1),\n",
       " (109, 1),\n",
       " (111, 1),\n",
       " (113, 1),\n",
       " (115, 1),\n",
       " (116, 1),\n",
       " (117, 1),\n",
       " (118, 1),\n",
       " (119, 1),\n",
       " (120, 1),\n",
       " (121, 1),\n",
       " (122, 1),\n",
       " (123, 1),\n",
       " (124, 1),\n",
       " (125, 1),\n",
       " (126, 1),\n",
       " (127, 1),\n",
       " (128, 1),\n",
       " (129, 1),\n",
       " (130, 1),\n",
       " (131, 1),\n",
       " (132, 1),\n",
       " (133, 1),\n",
       " (134, 1),\n",
       " (136, 1),\n",
       " (137, 1),\n",
       " (138, 1),\n",
       " (144, 1),\n",
       " (145, 1),\n",
       " (146, 1),\n",
       " (147, 1),\n",
       " (148, 1),\n",
       " (149, 1),\n",
       " (150, 1),\n",
       " (151, 1),\n",
       " (152, 1),\n",
       " (154, 1),\n",
       " (156, 1),\n",
       " (157, 1),\n",
       " (158, 1),\n",
       " (159, 1),\n",
       " (160, 1),\n",
       " (161, 1),\n",
       " (162, 1),\n",
       " (163, 1),\n",
       " (164, 1),\n",
       " (165, 1),\n",
       " (166, 1),\n",
       " (168, 1),\n",
       " (169, 1),\n",
       " (170, 1),\n",
       " (171, 1),\n",
       " (172, 1),\n",
       " (173, 1),\n",
       " (174, 1),\n",
       " (175, 1),\n",
       " (176, 1),\n",
       " (177, 1),\n",
       " (178, 1),\n",
       " (179, 1),\n",
       " (180, 1),\n",
       " (181, 1),\n",
       " (182, 1),\n",
       " (183, 1),\n",
       " (184, 1),\n",
       " (187, 1),\n",
       " (188, 1),\n",
       " (189, 1),\n",
       " (190, 1),\n",
       " (191, 1),\n",
       " (192, 1),\n",
       " (196, 1),\n",
       " (197, 1),\n",
       " (198, 1),\n",
       " (199, 1),\n",
       " (200, 1),\n",
       " (202, 1),\n",
       " (203, 1),\n",
       " (204, 1),\n",
       " (205, 1),\n",
       " (206, 1),\n",
       " (208, 1),\n",
       " (209, 1),\n",
       " (210, 1),\n",
       " (211, 1),\n",
       " (212, 1),\n",
       " (214, 1),\n",
       " (215, 1),\n",
       " (216, 1),\n",
       " (217, 1),\n",
       " (218, 1),\n",
       " (219, 1),\n",
       " (220, 1),\n",
       " (221, 1),\n",
       " (222, 1),\n",
       " (223, 1),\n",
       " (224, 1),\n",
       " (225, 1),\n",
       " (226, 1),\n",
       " (227, 1),\n",
       " (228, 1),\n",
       " (229, 1),\n",
       " (230, 1),\n",
       " (231, 1),\n",
       " (232, 1),\n",
       " (233, 1),\n",
       " (234, 1),\n",
       " (235, 1),\n",
       " (236, 1),\n",
       " (237, 1),\n",
       " (238, 1),\n",
       " (239, 1),\n",
       " (240, 1),\n",
       " (241, 1),\n",
       " (242, 1),\n",
       " (243, 1),\n",
       " (244, 1),\n",
       " (245, 1),\n",
       " (247, 1),\n",
       " (249, 1),\n",
       " (250, 1),\n",
       " (251, 1),\n",
       " (252, 1),\n",
       " (253, 1),\n",
       " (254, 1),\n",
       " (255, 1),\n",
       " (256, 1),\n",
       " (257, 1),\n",
       " (258, 1),\n",
       " (259, 1),\n",
       " (260, 1),\n",
       " (261, 1),\n",
       " (262, 1),\n",
       " (263, 1),\n",
       " (264, 1),\n",
       " (265, 1),\n",
       " (266, 1),\n",
       " (267, 1),\n",
       " (268, 1),\n",
       " (269, 1),\n",
       " (270, 1),\n",
       " (271, 1),\n",
       " (272, 1),\n",
       " (273, 1),\n",
       " (274, 1),\n",
       " (275, 1),\n",
       " (276, 1),\n",
       " (277, 1),\n",
       " (278, 1),\n",
       " (279, 1),\n",
       " (280, 1),\n",
       " (281, 1),\n",
       " (282, 1),\n",
       " (283, 1),\n",
       " (284, 1),\n",
       " (285, 1),\n",
       " (286, 1),\n",
       " (287, 1),\n",
       " (288, 1),\n",
       " (289, 1),\n",
       " (290, 1),\n",
       " (291, 1),\n",
       " (292, 1),\n",
       " (293, 1),\n",
       " (294, 1),\n",
       " (295, 1),\n",
       " (296, 1),\n",
       " (297, 1),\n",
       " (298, 1),\n",
       " (299, 1),\n",
       " (300, 1),\n",
       " (301, 1),\n",
       " (302, 1),\n",
       " (303, 1),\n",
       " (304, 1),\n",
       " (305, 1),\n",
       " (306, 1),\n",
       " (307, 1),\n",
       " (308, 1),\n",
       " (309, 1),\n",
       " (310, 1),\n",
       " (311, 1),\n",
       " (312, 1),\n",
       " (313, 1),\n",
       " (314, 1),\n",
       " (315, 1),\n",
       " (316, 1),\n",
       " (317, 1),\n",
       " (318, 1),\n",
       " (319, 1),\n",
       " (320, 1),\n",
       " (321, 1),\n",
       " (322, 1),\n",
       " (324, 1),\n",
       " (325, 1),\n",
       " (326, 1),\n",
       " (327, 1),\n",
       " (328, 1),\n",
       " (329, 1),\n",
       " (330, 1),\n",
       " (331, 1),\n",
       " (332, 1),\n",
       " (333, 1),\n",
       " (335, 1),\n",
       " (337, 1),\n",
       " (338, 1),\n",
       " (339, 1),\n",
       " (340, 1),\n",
       " (342, 1),\n",
       " (343, 1),\n",
       " (344, 1),\n",
       " (345, 1),\n",
       " (346, 1),\n",
       " (347, 1),\n",
       " (348, 1),\n",
       " (349, 1),\n",
       " (350, 1),\n",
       " (351, 1),\n",
       " (352, 1),\n",
       " (353, 1),\n",
       " (354, 1),\n",
       " (355, 1),\n",
       " (356, 1),\n",
       " (357, 1),\n",
       " (358, 1),\n",
       " (359, 1),\n",
       " (362, 1),\n",
       " (364, 1),\n",
       " (365, 1),\n",
       " (366, 1),\n",
       " (367, 1),\n",
       " (368, 1),\n",
       " (369, 1),\n",
       " (370, 1),\n",
       " (371, 1),\n",
       " (372, 1),\n",
       " (373, 1),\n",
       " (374, 1),\n",
       " (375, 1),\n",
       " (376, 1),\n",
       " (377, 1),\n",
       " (378, 1),\n",
       " (379, 1),\n",
       " (380, 1),\n",
       " (381, 1),\n",
       " (382, 1),\n",
       " (383, 1),\n",
       " (385, 1),\n",
       " (386, 1),\n",
       " (387, 1),\n",
       " (388, 1),\n",
       " (389, 1),\n",
       " (390, 1),\n",
       " (391, 1),\n",
       " (392, 1),\n",
       " (393, 1),\n",
       " (394, 1),\n",
       " (395, 1),\n",
       " (396, 1),\n",
       " (397, 1),\n",
       " (398, 1),\n",
       " (399, 1),\n",
       " (400, 1),\n",
       " (401, 1),\n",
       " (402, 1),\n",
       " (403, 1),\n",
       " (404, 1),\n",
       " (405, 1),\n",
       " (406, 1),\n",
       " (407, 1),\n",
       " (408, 1),\n",
       " (409, 1),\n",
       " (410, 1),\n",
       " (411, 1),\n",
       " (412, 1),\n",
       " (413, 1),\n",
       " (414, 1),\n",
       " (415, 1),\n",
       " (416, 1),\n",
       " (417, 1),\n",
       " (418, 1),\n",
       " (419, 1),\n",
       " (420, 1),\n",
       " (421, 1),\n",
       " (422, 1),\n",
       " (423, 1),\n",
       " (424, 1),\n",
       " (425, 1),\n",
       " (426, 1),\n",
       " (427, 1),\n",
       " (428, 1),\n",
       " (429, 1),\n",
       " (430, 1),\n",
       " (431, 1),\n",
       " (432, 1),\n",
       " (433, 1),\n",
       " (435, 1),\n",
       " (436, 1),\n",
       " (437, 1),\n",
       " (438, 1),\n",
       " (439, 1),\n",
       " (445, 1),\n",
       " (446, 1),\n",
       " (447, 1),\n",
       " (448, 1),\n",
       " (449, 1),\n",
       " (450, 1),\n",
       " (451, 1),\n",
       " (452, 1),\n",
       " (453, 1),\n",
       " (454, 1),\n",
       " (455, 1),\n",
       " (456, 1),\n",
       " (457, 1),\n",
       " (458, 1),\n",
       " (459, 1),\n",
       " (460, 1),\n",
       " (461, 1),\n",
       " (462, 1),\n",
       " (463, 1),\n",
       " (464, 1),\n",
       " (465, 1),\n",
       " (466, 1),\n",
       " (467, 1),\n",
       " (469, 1),\n",
       " (470, 1),\n",
       " (471, 1),\n",
       " (472, 1),\n",
       " (473, 1),\n",
       " (474, 1),\n",
       " (475, 1),\n",
       " (476, 1),\n",
       " (477, 1),\n",
       " (478, 1),\n",
       " (479, 1),\n",
       " (480, 1),\n",
       " (481, 1),\n",
       " (482, 1),\n",
       " (483, 1),\n",
       " (484, 1),\n",
       " (485, 1),\n",
       " (486, 1),\n",
       " (487, 1),\n",
       " (488, 1),\n",
       " (489, 1),\n",
       " (490, 1),\n",
       " (491, 1),\n",
       " (492, 1),\n",
       " (493, 1),\n",
       " (494, 1),\n",
       " (495, 1),\n",
       " (496, 1),\n",
       " (497, 1),\n",
       " (498, 1),\n",
       " (499, 1),\n",
       " (500, 1),\n",
       " (501, 1),\n",
       " (502, 1),\n",
       " (503, 1),\n",
       " (504, 1),\n",
       " (505, 1),\n",
       " (506, 1),\n",
       " (507, 1),\n",
       " (508, 1),\n",
       " (509, 1),\n",
       " (510, 1),\n",
       " (511, 1),\n",
       " (512, 1),\n",
       " (513, 1),\n",
       " (514, 1),\n",
       " (515, 1),\n",
       " (516, 1),\n",
       " (517, 1),\n",
       " (518, 1),\n",
       " (519, 1),\n",
       " (520, 1),\n",
       " (521, 1),\n",
       " (522, 1),\n",
       " (523, 1),\n",
       " (525, 1),\n",
       " (526, 1),\n",
       " (527, 1),\n",
       " (528, 1),\n",
       " (529, 1),\n",
       " (530, 1),\n",
       " (531, 1),\n",
       " (532, 1),\n",
       " (533, 1),\n",
       " (534, 1),\n",
       " (535, 1),\n",
       " (536, 1),\n",
       " (537, 1),\n",
       " (539, 1),\n",
       " (540, 1),\n",
       " (541, 1),\n",
       " (542, 1),\n",
       " (543, 1),\n",
       " (544, 1),\n",
       " (545, 1),\n",
       " (546, 1),\n",
       " (547, 1),\n",
       " (548, 1),\n",
       " (549, 1),\n",
       " (550, 1),\n",
       " (551, 1),\n",
       " (552, 1),\n",
       " (553, 1),\n",
       " (554, 1),\n",
       " (555, 1),\n",
       " (556, 1),\n",
       " (557, 1),\n",
       " (558, 1),\n",
       " (559, 1),\n",
       " (560, 1),\n",
       " (561, 1),\n",
       " (562, 1),\n",
       " (563, 1),\n",
       " (564, 1),\n",
       " (565, 1),\n",
       " (567, 1),\n",
       " (568, 1),\n",
       " (569, 1),\n",
       " (571, 1),\n",
       " (572, 1),\n",
       " (573, 1),\n",
       " (574, 1),\n",
       " (575, 1),\n",
       " (576, 1),\n",
       " (577, 1),\n",
       " (578, 1),\n",
       " (579, 1),\n",
       " (580, 1),\n",
       " (581, 1),\n",
       " (582, 1),\n",
       " (583, 1),\n",
       " (584, 1),\n",
       " (585, 1),\n",
       " (586, 1),\n",
       " (587, 1),\n",
       " (588, 1),\n",
       " (589, 1),\n",
       " (590, 1),\n",
       " (591, 1),\n",
       " (592, 1),\n",
       " (593, 1),\n",
       " (595, 1),\n",
       " (596, 1),\n",
       " (597, 1),\n",
       " (598, 1),\n",
       " (599, 1),\n",
       " (600, 1),\n",
       " (601, 1),\n",
       " (602, 1),\n",
       " (603, 1),\n",
       " (604, 1),\n",
       " (605, 1),\n",
       " (606, 1),\n",
       " (607, 1),\n",
       " (608, 1),\n",
       " (609, 1),\n",
       " (610, 1),\n",
       " (611, 1),\n",
       " (612, 1),\n",
       " (613, 1),\n",
       " (614, 1),\n",
       " (615, 1),\n",
       " (616, 1),\n",
       " (617, 1),\n",
       " (618, 1),\n",
       " (619, 1),\n",
       " (620, 1),\n",
       " (621, 1),\n",
       " (622, 1),\n",
       " (623, 1),\n",
       " (624, 1),\n",
       " (625, 1),\n",
       " (626, 1),\n",
       " (627, 1),\n",
       " (628, 1),\n",
       " (629, 1),\n",
       " (630, 1),\n",
       " (631, 1),\n",
       " (632, 1),\n",
       " (633, 1),\n",
       " (634, 1),\n",
       " (635, 1),\n",
       " (636, 1),\n",
       " (637, 1),\n",
       " (638, 1),\n",
       " (639, 1),\n",
       " (640, 1),\n",
       " (641, 1),\n",
       " (642, 1),\n",
       " (643, 1),\n",
       " (645, 1),\n",
       " (646, 1),\n",
       " (647, 1),\n",
       " (648, 1),\n",
       " (649, 1),\n",
       " (650, 1),\n",
       " (651, 1),\n",
       " (652, 1),\n",
       " (653, 1),\n",
       " (654, 1),\n",
       " (655, 1),\n",
       " (656, 1),\n",
       " (657, 1),\n",
       " (658, 1),\n",
       " (659, 1),\n",
       " (660, 1),\n",
       " (661, 1),\n",
       " (662, 1),\n",
       " (663, 1),\n",
       " (664, 1),\n",
       " (665, 1),\n",
       " (666, 1),\n",
       " (667, 1),\n",
       " (668, 1),\n",
       " (669, 1),\n",
       " (670, 1),\n",
       " (671, 1),\n",
       " (672, 1),\n",
       " (673, 1),\n",
       " (674, 1),\n",
       " (675, 1),\n",
       " (676, 1),\n",
       " (677, 1),\n",
       " (678, 1),\n",
       " (679, 1),\n",
       " (680, 1),\n",
       " (681, 1),\n",
       " (682, 1),\n",
       " (683, 1),\n",
       " (684, 1),\n",
       " (685, 1),\n",
       " (686, 1),\n",
       " (687, 1),\n",
       " (688, 1),\n",
       " (689, 1),\n",
       " (690, 1),\n",
       " (691, 1),\n",
       " (692, 1),\n",
       " (693, 1),\n",
       " (694, 1),\n",
       " (695, 1),\n",
       " (696, 1),\n",
       " (697, 1),\n",
       " (698, 1),\n",
       " (699, 1),\n",
       " (700, 1),\n",
       " (701, 1),\n",
       " (702, 1),\n",
       " (703, 1),\n",
       " (704, 1),\n",
       " (705, 1),\n",
       " (706, 1),\n",
       " (707, 1),\n",
       " (708, 1),\n",
       " (709, 1),\n",
       " (710, 1),\n",
       " (711, 1),\n",
       " (712, 1),\n",
       " (713, 1),\n",
       " (714, 1),\n",
       " (715, 1),\n",
       " (716, 1),\n",
       " (717, 1),\n",
       " (718, 1),\n",
       " (719, 1),\n",
       " (720, 1),\n",
       " (721, 1),\n",
       " (722, 1),\n",
       " (723, 1),\n",
       " (724, 1),\n",
       " (725, 1),\n",
       " (726, 1),\n",
       " (727, 1),\n",
       " (728, 1),\n",
       " (729, 1),\n",
       " (730, 1),\n",
       " (731, 1),\n",
       " (732, 1),\n",
       " (733, 1),\n",
       " (734, 1),\n",
       " (735, 1),\n",
       " (736, 1),\n",
       " (737, 1),\n",
       " (738, 1),\n",
       " (739, 1),\n",
       " (741, 1),\n",
       " (742, 1),\n",
       " (743, 1),\n",
       " (744, 1),\n",
       " (745, 1),\n",
       " (746, 1),\n",
       " (747, 1),\n",
       " (748, 1),\n",
       " (749, 1),\n",
       " (750, 1),\n",
       " (751, 1),\n",
       " (752, 1),\n",
       " (753, 1),\n",
       " (754, 1),\n",
       " (755, 1),\n",
       " (756, 1),\n",
       " (757, 1),\n",
       " (758, 1),\n",
       " (759, 1),\n",
       " (760, 1),\n",
       " (761, 1),\n",
       " (762, 1),\n",
       " (763, 1),\n",
       " (764, 1),\n",
       " (765, 1),\n",
       " (766, 1),\n",
       " (767, 1),\n",
       " (768, 1),\n",
       " (769, 1),\n",
       " (770, 1),\n",
       " (771, 1),\n",
       " (772, 1),\n",
       " (773, 1),\n",
       " (774, 1),\n",
       " (775, 1),\n",
       " (776, 1),\n",
       " (777, 1),\n",
       " (778, 1),\n",
       " (779, 1),\n",
       " (780, 1),\n",
       " (781, 1),\n",
       " (782, 1),\n",
       " (783, 1),\n",
       " (784, 1),\n",
       " (785, 1),\n",
       " (786, 1),\n",
       " (787, 1),\n",
       " (788, 1),\n",
       " (789, 1),\n",
       " (790, 1)]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 30\n",
      "70 13\n",
      "114 7\n",
      "201 4\n",
      "341 4\n",
      "17 3\n",
      "45 3\n",
      "185 3\n",
      "246 3\n",
      "524 3\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "c = Counter(cluster_ary)\n",
    "for group, cnt in c.most_common(10):\n",
    "    print(group , cnt)\n",
    "    #articles = titles[c == group]\n",
    "    #for news in articles:\n",
    "    #    print(news)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "791"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = []\n",
    "for e in c.most_common():\n",
    "    a.append(e[0])\n",
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['【狗仔偷拍】小嫻搬離何守正家租66坪房\\u3000月租6萬元', '許聖梅心疼小嫻被當空氣\\u3000爆何守正「有兩個女學員」',\n",
       "       '【動畫解盤】毒菇跳火線譙seafood\\u3000小嫻難瘦香菇', '【獨家】小嫻賣房求子\\u3000婆婆竟拒入籍何家',\n",
       "       '小心！在美結婚台灣沒登記\\u3000偷腥照樣能捉姦', '小嫻離婚導火線\\u3000拉何守正信妙禪',\n",
       "       '小嫻守正結婚在台沒登記\\u3000想離婚只有兩條路', '小嫻何守正想離婚\\u3000必須先做這件事！',\n",
       "       '【內幕動畫】小嫻婚變何守正姊反擊\\u3000不滿媽煮飯侍奉星媳婦', '小嫻婚變無徵兆\\u3000男星嘆：兩人向來出雙入對',\n",
       "       '【獨家內幕】太傷！小嫻被分手\\u3000何守正當小三面前攤牌',\n",
       "       '【小嫻離婚】何守正稱沒有遺憾\\u3000人妻女星超火「一嘴屁話」',\n",
       "       '【小嫻離婚】3大退讓人財兩失\\u3000求子花光430萬積蓄'], dtype='<U36')"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles_ary[cluster_ary == 70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<899x38653 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 128155 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = X[cluster_ary == 70].sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.squeeze(np.asarray(m2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['小嫻', '何守', '離婚', '結婚', '美國', '何家', '表示', '時間', '台灣', '婚姻'],\n",
       "      dtype='<U18')"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = np.array(vectorizer.get_feature_names())\n",
    "features[arr.argsort()[::-1][0:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3群\n",
      "新聞關鍵字：全能,神教,中國,宗教,宗教自由,組織,遭受,一個,迫害,研究\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "【特企】NGO聯名譴責中共迫害宗教信仰\n",
      "===========================\n",
      "第70群\n",
      "新聞關鍵字：小嫻,何守,離婚,結婚,美國,何家,表示,時間,台灣,婚姻\n",
      "【狗仔偷拍】小嫻搬離何守正家租66坪房　月租6萬元\n",
      "許聖梅心疼小嫻被當空氣　爆何守正「有兩個女學員」\n",
      "【動畫解盤】毒菇跳火線譙seafood　小嫻難瘦香菇\n",
      "【獨家】小嫻賣房求子　婆婆竟拒入籍何家\n",
      "小心！在美結婚台灣沒登記　偷腥照樣能捉姦\n",
      "小嫻離婚導火線　拉何守正信妙禪\n",
      "小嫻守正結婚在台沒登記　想離婚只有兩條路\n",
      "小嫻何守正想離婚　必須先做這件事！\n",
      "【內幕動畫】小嫻婚變何守正姊反擊　不滿媽煮飯侍奉星媳婦\n",
      "小嫻婚變無徵兆　男星嘆：兩人向來出雙入對\n",
      "【獨家內幕】太傷！小嫻被分手　何守正當小三面前攤牌\n",
      "【小嫻離婚】何守正稱沒有遺憾　人妻女星超火「一嘴屁話」\n",
      "【小嫻離婚】3大退讓人財兩失　求子花光430萬積蓄\n",
      "===========================\n",
      "第114群\n",
      "新聞關鍵字：專頁,影片,ig,網友,分享,cuteanimalsco,果粉,內容,絲團,圖片\n",
      "驚！　野生捕獲恐龍遛汪星人\n",
      "驚！　野生捕獲恐龍遛汪星人\n",
      "搔癢無尾熊 表情超enjoy\n",
      "【笑翻片】各種抓！動物止癢出奇招\n",
      "【保護片】就是不能下水！狗狗護小主人\n",
      "群狗亂舞開趴　畫面太「美」讓人不敢看\n",
      "超近看螞蟻食蜂蜜　竟感到療癒？\n",
      "===========================\n",
      "第201群\n",
      "新聞關鍵字：補助,台電,申請,立委,顏寬恒,一手,經費,萬元,報導,電申\n",
      "被爆罵、敲台電　顏寬恒「模糊反空污焦點」\n",
      "【反嗆片】被爆向台電申請補助又罵空污　顏寬恒：不會拿人手短\n",
      "「一手罵台電、一手敲台電」　經委會要台電公布立委補助資料\n",
      "被顏寬恒點名也跟台電要補助　蔡其昌：不應與空污混為一談\n",
      "===========================\n",
      "第341群\n",
      "新聞關鍵字：京華城,560,市府,計畫,容積率,都市,察院,積率,發局,提出\n",
      "【壹週刊】百億貸款將到期　柯P放容積解套京華城\n",
      "560%容積爭議　威京：只是拿回應有的\n",
      "京華城容積率增至560%案　北市都發局同意、待都委會審議\n",
      "及時雨！柯文哲放寬容積　京華城爽納百億\n",
      "===========================\n",
      "第17群\n",
      "新聞關鍵字：桃園,宿舍,失聯,汽車,工廠,火勢,矽卡,現場,逃出,員工\n",
      "【不斷更新】桃園工廠惡火撲滅 6人仍失聯宿舍內發現一堆白骨\n",
      "桃園工廠火勢熄滅　員工宿舍內發現一堆白骨\n",
      "汽車用品大廠「矽卡」燒毀　資本額達2億\n",
      "===========================\n",
      "第45群\n",
      "新聞關鍵字：地區,北部,氣象局,高溫,影響,水氣,中南部,半部,玉山,山區\n",
      "後天入冬最強冷空氣來襲　低溫下探12℃\n",
      "像灑了糖霜！玉山今晨降雪　積雪0.5公分\n",
      "輕颱啟德恐生成　入冬最強冷空氣周末來襲下探11℃（動畫）\n",
      "===========================\n",
      "第185群\n",
      "新聞關鍵字：天堂,遊戲,玩家,伺服器,橘子,開服,下載,12,上線,網友\n",
      "【崩潰動畫】《天堂M》斷線50次　玩家罵聲連連\n",
      "台戰豪砸40萬抽寶一場空　怒批《天堂M》根本錢坑\n",
      "《天堂M》超過116萬人登入　50個伺服器仍塞到爆\n",
      "===========================\n",
      "第246群\n",
      "新聞關鍵字：px,高雄,font,text,000000,height,銀行,12,表示,line\n",
      "【世界一瞬間】日本扭蛋風潮\n",
      "曾銘宗爆「好大的高雄銀」　高雄銀嗆：保留法律追訴權\n",
      "黃國昌爆高雄銀替慶富不實增資　公司：非事實\n",
      "===========================\n",
      "第524群\n",
      "新聞關鍵字：三中,建新,中投,公司,報導,張哲琛,集團,錄音,英九,檢方\n",
      "三中案馬英九圖利中時老闆？　北檢：不評論\n",
      "三中案祕錄光碟！恐扳倒馬英九　當年他錄的\n",
      "三中案500萬禮盒照　馬英九一句話成關鍵\n",
      "===========================\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "c = Counter(cluster_ary)\n",
    "\n",
    "for group, cnt in c.most_common(10):\n",
    "    print('第{}群'.format(group))\n",
    "    articles = titles_ary[cluster_ary == group]\n",
    "    m2 = X[cluster_ary == group].sum(axis = 0)\n",
    "    arr = np.squeeze(np.asarray(m2))\n",
    "    features = np.array(vectorizer.get_feature_names())\n",
    "    keywords = features[arr.argsort()[::-1][0:10]]\n",
    "    print('新聞關鍵字：{}'.format(','.join(keywords)))\n",
    "    for news in articles:\n",
    "        print(news)\n",
    "    print('===========================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文章分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "news = pandas.read_excel('https://raw.githubusercontent.com/ywchiu/ctbc/master/data/20171214news.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "titles = []\n",
    "tags   = []\n",
    "for rec in news[news['category'].isin(['娛樂','社會'])].iterrows():\n",
    "    corpus.append(' '.join(jieba.cut(rec[1]['content'])))\n",
    "    titles.append(rec[1]['title'])\n",
    "    tags.append(rec[1]['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<247x13883 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 32161 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "train_data, test_data, train_title, test_title, train_tag, test_tag =train_test_split(X, titles, tags, test_size = 0.3, random_state= 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(172, 13883)"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75, 13883)"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB(alpha=0.01)\n",
    "clf.fit(train_data,train_tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = clf.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9866666666666667"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(test_tag == predicted) / len(test_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['娛樂' '社會']\n",
      "[[32  0]\n",
      " [ 1 42]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(clf.classes_)\n",
    "m = confusion_matrix(test_tag, predicted)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['【K律師論點】離婚＝失敗？\\u3000K律師這麼說'], dtype='<U36')"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_titles = np.array(test_title)\n",
    "np_titles[test_tag!= predicted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "s = pickle.dumps(clf)\n",
    "with open('model.pkl', 'wb') as f:\n",
    "    f.write(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = pickle.loads(open('model.pkl', 'rb').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf2.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = '''\n",
    "5566成員孫協志、王仁甫、許孟哲三人合體，將在明年2月23日攻蛋，昔日戰友王少偉卻缺席這場盛會，今售票記者會，5566再度被問起究竟有何過不去的嫌隙或是心結，讓別具意義的攻蛋演唱會沒了王少偉？王仁甫表示，「我自己也覺得有點遺憾，如果能四人最好。但看王少偉本身啦，看他的意願。」私下都未主動邀王少偉？三人異同口聲表示，主辦單位沒邀, 演唱會一開始就設定是找節目《飢餓遊戲》的主持人，但也坦承私下確實未主動邀王少偉，孫協志則稱「大家是十多年的兄弟，沒有存在心結問題，人到一個年紀，基本上有自己的想法，做法，抱持一個尊重的態度，你想這麼做，盡可能尊重。」\n",
    "'''\n",
    "new_corpus = ' '.join(jieba.cut(article))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = vectorizer.transform([new_corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x13883 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 47 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['娛樂'], dtype='<U2')"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2.predict(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64.0, '男子', 0.0, 64.0)\n",
      "(38.0, '突發', 0.0, 38.0)\n",
      "(34.0, '機車', 0.0, 34.0)\n",
      "(32.0, '警員', 0.0, 32.0)\n",
      "(31.0, '法官', 0.0, 31.0)\n",
      "(30.2, '警方', 4.0, 151.0)\n",
      "(30.0, '少年', 0.0, 30.0)\n",
      "(30.0, '派出所', 0.0, 30.0)\n",
      "(29.0, '進行', 0.0, 29.0)\n",
      "(25.0, '女友', 0.0, 25.0)\n",
      "(25.0, '新北', 0.0, 25.0)\n",
      "(24.0, '分局', 0.0, 24.0)\n",
      "(24.0, '警察', 0.0, 24.0)\n",
      "(23.0, '審團', 0.0, 23.0)\n",
      "(23.0, '時許', 0.0, 23.0)\n",
      "(23.0, '消防局', 0.0, 23.0)\n",
      "(22.0, '包商', 0.0, 22.0)\n",
      "(22.0, '報警', 0.0, 22.0)\n",
      "(22.0, '專線', 0.0, 22.0)\n",
      "(22.0, '被告', 0.0, 22.0)\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "coef_features_c1_c2 = []\n",
    "\n",
    "for index, features in enumerate(zip(vectorizer.get_feature_names(), \\\n",
    "                        clf.feature_count_[0], clf.feature_count_[1])):\n",
    "    feat,c1,c2 = features\n",
    "    coef_features_c1_c2.append(tuple([c2/(c1 + 1), feat, c1, c2]))\n",
    "\n",
    "for i in sorted(coef_features_c1_c2, key = operator.itemgetter(0), reverse=True)[0:20]:\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25.75, '小嫻', 103.0, 3.0)\n",
      "(21.0, '演出', 21.0, 0.0)\n",
      "(19.0, '何家', 19.0, 0.0)\n",
      "(15.0, '女星', 15.0, 0.0)\n",
      "(13.0, '最佳', 13.0, 0.0)\n",
      "(12.5, '婆婆', 25.0, 1.0)\n",
      "(12.0, '明年', 12.0, 0.0)\n",
      "(12.0, '節目', 12.0, 0.0)\n",
      "(12.0, '電影', 12.0, 0.0)\n",
      "(11.0, '信妙', 11.0, 0.0)\n",
      "(11.0, '娛樂', 11.0, 0.0)\n",
      "(11.0, '巴黎', 11.0, 0.0)\n",
      "(10.0, '101', 10.0, 0.0)\n",
      "(10.0, '何守', 80.0, 7.0)\n",
      "(9.0, '不同', 9.0, 0.0)\n",
      "(9.0, '妙禪', 9.0, 0.0)\n",
      "(9.0, '媳婦', 9.0, 0.0)\n",
      "(9.0, '毒菇', 9.0, 0.0)\n",
      "(9.0, '法國', 9.0, 0.0)\n",
      "(8.666666666666666, '婚變', 26.0, 2.0)\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "coef_features_c1_c2 = []\n",
    "\n",
    "for index, features in enumerate(zip(vectorizer.get_feature_names(), \\\n",
    "                        clf.feature_count_[0], clf.feature_count_[1])):\n",
    "    feat,c1,c2 = features\n",
    "    coef_features_c1_c2.append(tuple([c1/(c2 + 1), feat, c1, c2]))\n",
    "\n",
    "for i in sorted(coef_features_c1_c2, key = operator.itemgetter(0), reverse=True)[0:20]:\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 抓取電影評論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "res = requests.get('https://movies.yahoo.com.tw/movieinfo_review.html/id=8136')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(res.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_reviews = []\n",
    "for rec in soup.select('#form_good1'):\n",
    "    comments = rec.select('span')[-1].text\n",
    "    scores   = rec.select_one('input[name=\"score\"]').get('value')\n",
    "    movie_reviews.append({'scores':scores, 'comments':comments})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "moviedf = pandas.DataFrame(movie_reviews)\n",
    "#moviedf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "moviedf = pandas.read_excel('https://raw.githubusercontent.com/ywchiu/ctbc/master/data/yahoo_movie.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "content    1112\n",
       "stars      1112\n",
       "title      1112\n",
       "status     1112\n",
       "dtype: int64"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moviedf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "tags   = []\n",
    "for rec in moviedf[moviedf['status'].isin(['good', 'bad'])].iterrows():\n",
    "    corpus.append(' '.join(jieba.cut(rec[1]['content'])))\n",
    "    tags.append(rec[1]['status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<980x5900 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 16897 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, tags, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB(alpha = 0.01)\n",
    "clf.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = clf.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7517006802721088"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "accuracy_score(test_y, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bad' 'good']\n"
     ]
    }
   ],
   "source": [
    "print(clf.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 87,  30],\n",
       "       [ 43, 134]])"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test_y, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15.0, '奧創', 0.0, 15.0)\n",
      "(13.0, '希望', 0.0, 13.0)\n",
      "(13.0, '影評', 0.0, 13.0)\n",
      "(13.0, '當然', 0.0, 13.0)\n",
      "(12.0, '感動', 0.0, 12.0)\n",
      "(11.0, '相當', 0.0, 11.0)\n",
      "(11.0, '細節', 0.0, 11.0)\n",
      "(10.0, 'the', 0.0, 10.0)\n",
      "(10.0, '效果', 0.0, 10.0)\n",
      "(10.0, '還不錯', 1.0, 20.0)\n",
      "(9.5, '不同', 1.0, 19.0)\n",
      "(9.0, 'be', 0.0, 9.0)\n",
      "(9.0, 'https', 0.0, 9.0)\n",
      "(9.0, '元素', 0.0, 9.0)\n",
      "(9.0, '鷹眼', 0.0, 9.0)\n",
      "(8.75, '不錯', 3.0, 35.0)\n",
      "(8.0, '之間', 0.0, 8.0)\n",
      "(8.0, '參考', 0.0, 8.0)\n",
      "(8.0, '大戰', 0.0, 8.0)\n",
      "(8.0, '如此', 0.0, 8.0)\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "coef_features_c1_c2 = []\n",
    "\n",
    "for index, features in enumerate(zip(vectorizer.get_feature_names(), \\\n",
    "                        clf.feature_count_[0], clf.feature_count_[1])):\n",
    "    feat,c1,c2 = features\n",
    "    coef_features_c1_c2.append(tuple([c2/(c1 + 1), feat, c1, c2]))\n",
    "\n",
    "for i in sorted(coef_features_c1_c2, key = operator.itemgetter(0), reverse=True)[0:20]:\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_comment = ['聚了一堆大咖,表情都超不到位,超級冷場, 劇情超爛而且都超扯蛋,而且很多地方都交待不清,隨便虎弄帶過 ,最後也草草結尾,到底再演什麼鬼', '娛樂效果十足，超棒的演員角色，希望有續作' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus2 = []\n",
    "for ele in new_comment:\n",
    "    #print(ele)\n",
    "    corpus2.append(' '.join(jieba.cut(ele)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "s = pickle.dumps(vectorizer)\n",
    "with open('vectorizer.pkl', 'wb') as f:\n",
    "    f.write(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer2 = pickle.loads(open('vectorizer.pkl', 'rb').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_X = vectorizer2.transform(corpus2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['bad', 'good'], dtype='<U4')"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(predict_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = clf.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7244897959183674"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "accuracy_score(test_y, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
