{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark \n",
    "\n",
    "data = [1,2,3,4,5]\n",
    "rdd = sc.parallelize(data, 4) \n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "lines = sc.textFile('file:/tmp/trump.txt') \n",
    "lines.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "def timesTwo(e):\n",
    "    return e * 2\n",
    "    \n",
    "rdd = sc.parallelize([1, 2, 3, 4]) \n",
    "rdd2 = rdd.map(timesTwo)\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "rdd = sc.parallelize([1, 2, 3, 4]) \n",
    "rdd2 = rdd.map(lambda e: e * 2)\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "rdd2 = rdd.filter(lambda x: x % 2 == 0)\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "rdd = sc.parallelize([1,4,2,2,3])\n",
    "rdd2 = rdd.distinct()\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "rdd=sc.parallelize([1,2,3])\n",
    "rdd2 = rdd.map(lambda x:[x,x+5])\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "rdd2 = rdd.flatMap(lambda x:[x,x+5])\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd=sc.parallelize([1,2,3])\n",
    "rdd.reduce(lambda x,y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd=sc.parallelize([5,3,1,2]) \n",
    "rdd.takeOrdered(3,lambda s:-1*s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key-Value RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd  = sc.parallelize([(1,2), (3,4), (3,6)]) \n",
    "rdd2 = rdd.reduceByKey(lambda a, b: a + b)\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 = sc.parallelize([(1,'a'), (2,'c'), (1,'b')]) \n",
    "rdd3 = rdd2.sortByKey()\n",
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 = sc.parallelize([(1,'a'), (2,'c'), (1,'b')]) \n",
    "rdd3 = rdd2.groupByKey()\n",
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcast, Accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "broadcastVar = sc.broadcast([1, 2, 3])\n",
    "broadcastVar.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "accum = sc.accumulator(0) \n",
    "rdd = sc.parallelize([1, 2, 3, 4])\n",
    "\n",
    "def f(x):\n",
    "    global accum \n",
    "    accum += x\n",
    "    \n",
    "rdd.foreach(f) \n",
    "\n",
    "accum.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "rdd = sc.textFile('/tmp/trump.txt', 4)\n",
    "rdd2 = rdd.flatMap(lambda e: e.lower().split())\n",
    "dict = rdd2.countByValue()\n",
    "dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie Likes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "lines = sc.textFile(\"file:///tmp/u.data\")\n",
    "#lines.take(3)\n",
    "movies = lines.map(lambda e: (e.split()[1],1) )\n",
    "#movies.take(3)\n",
    "movieCounts = movies.reduceByKey(lambda x,y: x+ y)\n",
    "res = movieCounts.sortBy(lambda a : -a[1])\n",
    "res.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "def loadMovieNames(): \n",
    "    movieNames = {}\n",
    "    with open('/tmp/u.item', 'r',encoding = 'latin1') as f:\n",
    "        for line in f:\n",
    "            fields = line.strip().split('|')\n",
    "            movieNames[fields[0]] = fields[1] \n",
    "    return movieNames\n",
    "    \n",
    "nameDict = sc.broadcast(loadMovieNames())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "#loadMovieNames()\n",
    "res = movieCounts.sortBy(lambda a: -a[1])\n",
    "res2 = res.map(lambda e: (nameDict.value.get(e[0]), e[1]))\n",
    "res2.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "data_file = '/tmp/ratings.txt'\n",
    "raw_data  = sc.textFile(data_file, 4)\n",
    "raw_data.take(3)\n",
    "\n",
    "header = raw_data.first()\n",
    "header\n",
    "\n",
    "skip_data = raw_data.filter(lambda line: line != header)\n",
    "skip_data.take(3)\n",
    "\n",
    "\n",
    "csv_data = skip_data.map(lambda l: l.split(\"::\"))\n",
    "csv_data.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "from pyspark.sql import Row\n",
    "row_data = csv_data.map(\n",
    "        lambda p: \n",
    "            Row(\n",
    "                userid=p[0], \n",
    "                itemid=p[1], \n",
    "                rating=int(p[2]) \n",
    "            )\n",
    ")\n",
    "row_data.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "a = [{'userid':0, 'itemid':0, 'rating':4},\n",
    " {'userid':0, 'itemid':1, 'rating':5}\n",
    "]\n",
    "# padas df\n",
    "import pandas\n",
    "pandas_df = pandas.DataFrame(a)\n",
    "pandas_df\n",
    "pandas_df[['itemid', 'rating']].groupby('itemid')['rating'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "df = sqlContext.createDataFrame(row_data)\n",
    "df.take(3)\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "# select itemid, AVG(rating) from df gorup by itemid  \n",
    "df.select(\"itemid\", \"rating\").groupBy(\"itemid\").avg().show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "df.registerTempTable(\"ratings\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "ratings_data = sqlContext.sql(\"\"\"\n",
    "    SELECT itemid,AVG(rating) FROM ratings GROUP BY itemid\n",
    "\"\"\") \n",
    "ratings_data.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SparkSQL to Analyze Movie Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "movies = sc.textFile('file:///tmp/u.item', 4)\n",
    "movies_data = movies.map(lambda l:l.split('|')) \n",
    "#movies_data.take(3)\n",
    "movies_row_data = movies_data.map(lambda p:\n",
    "    Row(movieid=p[0], moviename=p[1] ) )\n",
    "movies_row_data.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "movies_df = sqlContext.createDataFrame(movies_row_data)\n",
    "movies_df.registerTempTable(\"movies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "ratings = sc.textFile('file:///tmp/u.data', 4) \n",
    "ratings_data = ratings.map(lambda l:l.split()) \n",
    "ratings_row_data = ratings_data.map(lambda p:\n",
    "    Row( userid=p[0], movieid=p[1], rating=int(p[2]) ) )\n",
    "\n",
    "ratings_row_data.take(4)\n",
    "df = sqlContext.createDataFrame(ratings_row_data) \n",
    "df.registerTempTable(\"ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "best_movies = sqlContext.sql(\"\"\"\n",
    "SELECT moviename,avg(rating) as avg_rating, count(1) as cnt \n",
    "FROM movies INNER JOIN ratings ON ratings.movieid = movies.movieid \n",
    "GROUP BY moviename \n",
    "ORDER by cnt DESC LIMIT 10\n",
    "\"\"\") \n",
    "best_movies.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "rawData = sc.textFile(\"/tmp/u.data\") \n",
    "rawData.first() \n",
    "\n",
    "rawRatings = rawData.map(lambda e: e.split()) \n",
    "rawRatings.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "from pyspark.sql import Row\n",
    "ratingsRDD = rawRatings.map(\\\n",
    "    lambda p: Row(userId=int(p[0]), \\\n",
    "    movieId=int(p[1]), \\\n",
    "    rating=float(p[2]), \\\n",
    "    timestamp=int(p[3])))\n",
    "    \n",
    "ratings = spark.createDataFrame(ratingsRDD)\n",
    "(training, test) = ratings.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "from pyspark.ml.recommendation import ALS\n",
    "als = ALS(rank=50, maxIter=10, regParam=0.01, \\\n",
    "    userCol=\"userId\", itemCol=\"movieId\", \\\n",
    "    ratingCol=\"rating\")\n",
    "model = als.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "model.userFactors.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "model.itemFactors.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "userRecs = model.recommendForAllUsers(10)\n",
    "userRecs.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "movieRecs = model.recommendForAllItems(10)\n",
    "movieRecs.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
